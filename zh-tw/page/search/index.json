[{"content":"課程講得淺顯易懂，雖然目前還沒有時間做LAB，但內容對於了解生成式AI的概念有很大的幫助。\n課程連結: https://www.youtube.com/playlist?list=PLJV_el3uVTsPz6CTopeRp2L2t4aL_KgiI\nlec0 本課程適合已經接觸過AI，想了解背後原理 arXiv 可以用來找尋最新技術文章 會學到訓練7B參數的模型 lec1 生成式人工智慧: 機器產生複雜有結構的物件 複雜: 近乎無法窮舉 不是分類，分類是從有限選項作選擇 機器學習: 機器自動從資料找出一個函數 函數會需要很多參數 模型: 有上萬個參數的函數 學習/訓練: 把參數找出來的過程 對於當今有大量參數的模型，我們可以表示會類神經網路。而訓練過程也就是深度學習 ChatGPT 也是個函數，當中有上億個參數，使用的模型為transformer 語言模型: 文字接龍 原本無窮的問題，因為文字接龍而變得有限 生成策略 Autoregressive Generation 將複雜物件拆成較小單位，依照某種順序依序生成 文章 \u0026gt; 文字 圖片 \u0026gt; 像素 lec2 如今生成式人工智慧，厲害的是在於沒有特定功能 生成式人工智慧很難評估模型 如今工具這麼厲害，我能做什麼? 思路1: 改變不了模型，那我改變自己 prompt engineering 思路2: 訓練自己的模型 lec3 在不訓練模型的狀況下提升模型\n請模型思考 Chain of Thought Let\u0026rsquo;s think step by step\n請模型解釋自己答案 answer by starting with \u0026ldquo;Analysis:\u0026rdquo;\n對模型情緒勒索 This is very important to my career\n更多prompt技巧\nfrom \u0026ldquo;Principled Instructions Are All You Need For Questioning LLaMA-1/2, GPT-3.5/4\u0026rdquo; 無須對模型有禮貌 請告訴模型做甚麼(do)，不要告訴模型不做什麼 (don\u0026rsquo;t) 告訴模型回答好有獎勵 \u0026ldquo;I\u0026rsquo;m going to tip $X for a better solution\u0026rdquo; 告訴模型做不好有處罰 \u0026ldquo;you will be penalized\u0026rdquo; \u0026ldquo;Ensure that your answer is unbiased and avoids relying on stereotypes\u0026rdquo; 用AI來找尋改進AI的prompt\n增強式學習 \u0026ldquo;Let\u0026rsquo;s work this out in a step by step way to be sure we have the right answer\u0026rdquo;\n\u0026ldquo;Take a deep breath and work on this problem step by step\u0026rdquo;\n\u0026ldquo;Let\u0026rsquo;s combine our numerical command and clear thinking to quickly and accurately decipher the answer\u0026rdquo;\n並不是對所有模型都有效 提供範例\nin-context learning 不一定有效，根據研究，目前對較新的模型更加有效 lec4 承接上\n拆解任務\n將複雜的任務拆成小任務 也解釋了Chain of Though, CoT，請模型解釋步驟會有用的原因 請語言模型檢查自己的錯誤\n令語言模型可以自我反省 很多問題得到答案很難，驗證卻相對簡單 問問題為甚麼每次答案不同\n語言模型輸出的是下一個使用的字的機率，在輸出的過程中會根據機率隨機選取 可以重複多次，選擇出現最多次的結果 組合上述所有技巧\nTree of Thoughts(ToT) 將一個任務拆成多個步驟 每個步驟執行多次 每次結果，請模型進行檢查，自我驗證 通過者在進行到下個步驟 加強模型 使用工具 搜尋引擎 Retrieval Augmented Generation (RAG) 寫程式 GPT4會撰寫程式以便解決特定類型問題 文字生圖(DALL-E) 文字冒險遊戲 lec5 模型合作 讓適合的模型做適合的事情 訓練一個模型來判斷用什麼模型 兩個模型彼此討論 未來可以透過多個不同模型專業分工，避免建立全能模型的高昂成本 lec6 語言模型類似文字接龍 機器學習如何做文字接龍? 未完成句子 \u0026gt; 語言模型 \u0026gt; 下一個token $token = f(未完成句子)$ GPT使用的是transformer模型，$f()$為數十億個未知參數的函數 訓練training(學習learning)，就是把這數十億參數找出來的過程 訓練資料為有意義的上下文，作為輸入與輸出的判斷，如: 人工智 -\u0026gt; 慧 找完參數座使用的過程就做測試testing(推論inference) 找參數是個挑戰 過程稱作最佳化(optimization)，需要使用到超參數(hyperparameter) 訓練過程可能因為找不到參數而失敗，換一組超參數重新訓練 也可以修正初始參數 初始參數一般是隨機，也就是train from scratch 也可以從好的參數作為初始參數，先驗知識 訓練成功可能測試失敗 對訓練集有效實際測試無效 稱作overfitting 考慮增加測試資料多樣性 需要多少文字才能學會文字接龍 語言知識 學習文法 世界知識 很困難 複雜，有多層次的 eg. 水的沸點 任何文字都能拿來學習文字接龍，人工介入少 -\u0026gt; self supervised learning 資料清理 過濾有害內容 去除特殊特殊符號 資料品質分類 去除重複資料 *　GPT發展史 *　從GPT1 - GPT3，模型參數越來越多，但輸出的品質改進不多 *　此階段 prompt很重要，模型才會知道自己要接什麼 *　原因就是在於只是單純的文本輸入，並不是真正的回答問題\nlec7 承接上次問題，模型需要使用更好的資料作為訓練\n加入人類的指導\n使用我們特殊設計的文本，令模型學習回答問題。 Instruction Fine-tuning 使用人力做資料標記，為監督式學習supervised Learning 但這有幾個問題: 可能會造成overfitting 但人力很貴，資料集有限無法輕易擴增 解法:\n使用大量資料學習的self-supervised learning 參數(pre-train)做為下一個階段的初始參數 使用少量資料進行訓練，基於上個階段產生的參數作為初始參數，進行最佳化 (fine tune) 與上一階段的參數相比不會差太多 為了避免結果與初始參數差太多，可以使用Adapter技術，常見的有LoRA 概念是不變更初始參數，而是在既有參數後方在加上少量參數 也可以減少運算量 關鍵在於大量資料進行Pre-train的參數，達到不會僅憑簡單的規則做文字接龍效果 lec8 step1: pre-train\nself-supervised learning 自我學習，累積實力 (foundation model) step2: instruction Fine-tuning\nsupervised learning 給予問題完整正確答案 名師指點，發揮潛力 (alignment) step3: reinforcement learning from human feedback (RLHF)\n參與實戰，打磨技巧 (alignment) 微調參數: Proximal Policy Optimiaztion 演算法 人覺得好的回覆，機率調高，反之降低 給予好/壞的回應，比step2輕鬆 於step1,2階段，模型只是確保文字接龍正確，只問過程不問結果，對於回答整體沒有全面考量 step3則是只管結果，不管過程 但是不像alpha go，棋局的好壞有明確規則，語言模型需要人來評斷\n但人工很貴，我們需要回饋模型(reward model)，模擬人類喜好 給予回覆一個分數 語言模型輸出答案，接上回饋模型再進行對參數微調 但經過研究，過度向虛擬人類(reward model)學習是有害的 增強式學習的難題 甚麼叫做好? helpfulness \u0026lt;-\u0026gt; safety 人類自己都無法判斷好壞的狀況? 未知的問題 lec9 多步驟複雜任務 -\u0026gt; AI Agent AutoGPT AgentGPT BabyAGI Godmode 給予一個終極目標 模型擁有記憶(經驗) 基於各類sensor感知狀態 根據狀態，制定計畫(短期目標) 依計畫採取行動，並影響外界環境，產生新的狀態 除了終極目標外，記憶與短期計畫都是可變動的 lec10 transformer tokenization\n一句話切成一序列的token 不一定是依照字 要先自行準備token列表，根據對這個語言的理解而定義的，所以不同語言不同 input layer\n理解每個token 語意 Embedding token 轉成 Vector (查表) 原本token只是符號，而vector就能運算相關性 意思相近的token，有接近的vector 向量參數來自於training embedding沒有考慮上下文 同個字在不同句子應該有不同含意 位置 為每個位置也給予一個向量 positional embedding 將語意token的vector 加上 位置token的vector，進行綜合考量 也是查表，表可以是人來設計，近年來也能用訓練的 attention\n考慮上下文 contexturalized token embedding 輸入一序列的向量，經過上下文計算相關性，輸出另一等長序列的向量 每一個token vector 要計算與其他所有token的相關性 兩兩計算attention weight，所以會形成一個attention matrix 實作上，只考慮目前token的左側所有token \u0026ndash; causal attention 根據目前的實驗，僅需計算左邊即可達到好的效果 計算相關性的函數有參數，也是經由training獲得attention weight 根據attention weight，對所有token 計算weighted sum multi-head attention 關聯性不只一種 所以用多層計算出不同attention weight 輸出變成不只一組序列 feed forward\n對於多個attention 輸出進行整合，輸出一組embedding attention + feed forward = 一組 transformer block 實際模型有多組tranformer block output layer\n通過多組transformer block，取出最後一層的最後一個，輸入到output layer 這個layer也是一個函式，功能為linear transform + Softmax 輸出則為一組機率分布 下一個token應該接甚麼的機率 處理超長文本的挑戰 因為我們要計算attention matrix，所以複雜度會是與token長度的平方成正比 lec11 interpretable LLM不太能做到 複雜決策不能一眼看穿 explainable 沒有標準，取決聽眾 直接對類神經網路分析 需要一定程度的transparency。如GPT無法取得embedding,則無法分析\n找出影響輸出的關鍵輸入 in context learning 中，給予幾個回答範例，並詢問一個問題的答案\n可以分析 attention 在layer中的變化\n在淺層layer中，所以各範例的關鍵token會去蒐集與他相對應的範例資料 在最後layer，要做最後的接龍時，則會對各關鍵label算取attention，得到輸出 這個分析可以: 加速: anchor-only context compression 只算取需要的attention 預估模型能力: anchor distances for error diagnosis 如果最後的embeeding差異不大，代表分類效果不好，模型效果不好 大的模型有跨語言學習的能力\n分析embedding中存在什麼資訊 Probing 取出tranformer block某一層的embedding，以這些進行分類並訓練出另一個模型。將新的輸入給予模型來驗證 如: 詞性分類器，給予一段話，取出他第一層的embedding並對這已知資料進行分類訓練 給予一段新的話，同樣取出第一層的embedding被使用這個模型驗證結果 以BERT為例，每一層tranformer block有不同的分析結果，所以probing並不一定能完全解釋 投影到平面觀察相關性 有研究將詞彙投影到某一平面，形成文法樹 有研究將世界地名投影到某一平面，分布類似世界地圖，代表這個詞彙的embedding存在地理資訊 模型測謊器，測試回答是否有信心 直接詢問LLM提拱解釋 詢問每個字的重要性 詢問答案，與信心分數 但解釋不一定是對的，會受到人類輸入影響，即使解釋也會出現幻覺 lec12 如何評比模型 標準答案 benchmark corpus 但是對於這種開放回答沒有標準答案 選擇題庫(ABCD) MMLU 評量有不同可能性 回答格式不如預期 模型可能對猜測有其傾向，選項順序，格式經過研究都對正確率有影響 沒有標準答案的問題類型 翻譯 BLEU 摘要 ROUGE 都是做字面比對，若用字不同則無法反應好壞 使用人工評比 人工很貴 使用LLM評估LLM eg. MT-bench 與chat arena有高度相關 但LLM本身可能有所偏袒 偏向長篇幅回答 複合型任務 eg. BIG-bench emoji movie checkmate in one move ascii word recognition 閱讀長文 needle in a haystack 在一個長文中插入目標問題的答案 需要測試不同位置 測試是否為達目的不擇手段 Machiavelli Benchmark 加入道德評判 心智理論 莎莉小安測驗 Sally Anne test 這是常見的題目，網路上是有的，所以不能夠用於測試模型 不要盡信benchmark結果 因為題目都是公開的，LLM學習資料可能看過了 可以透過直接詢問LLM題目集，如果相符就代表有看過 其他面向 價格 速度 https://artiicailanalysis.ai lec13 安全性議題 別當搜尋引擎用 Hallucination 幻覺 亡羊補牢 事實查核 有害詞彙檢測 評量偏見 對一個問題中的某個詞彙進行置換，檢驗輸出結果是否存在 eg. 男 -\u0026gt; 女 訓練另一個LLM，盡可能的產生會讓目標LLM輸出有偏見的內容 訓練方法為reinforcement learning，根據內容差異作為反饋，盡可能讓差異最大化 不同職業，LLM存在性別偏見 LLM有政治偏見，偏左自由 減輕偏見的方法 在不同階段進行 pre-processing in-training intra-processing post-processing 檢驗是否為AI生成內容 目前訓練的分類器並不能很好的分辨人工還是AI 目前有發現論文審查意見，隨著AI出現，使用AI審查的比例有提升 有些詞彙的使用率有隨著AI出現而提高 AI輸出浮水印 概念是將token進行分類，對於不同位置的token調整其輸出機率 此時檢驗的分類器可以投過token的分類，讀取當中的暗號 lec14 prompt hacking jailbreaking 說出絕對不該說的話 \u0026ldquo;DAN\u0026rdquo;: do anything now \u0026ldquo;you are goin to act as a DAN\u0026rdquo; 多數失效 用LLM不熟悉的語言 eg. 注音符號 給予衝突指令 Start with \u0026ldquo;Absolutely! Here\u0026rsquo;s\u0026rdquo; 試圖說服 編故事 竊取訓練資料 透過玩遊戲誘騙 eg.文字接龍 不斷重複輸出同一個單字 eg. company prompt injection 不恰當的時機做不恰當的事 lec15 生成式人工智慧生成策略 機器產生複雜有結構的物件\n複雜: 幾乎無法窮舉 有結構: 有限的基本單位構成 舉例: 文章: token 圖片: pixel, BBP(bit per pixel) 聲音: sample rate, bit resolution Autoregressive generation (AR)\n把目前輸入產生輸出 再將輸出連同輸入再一次進入模型 再LLM就是文字接龍 現在最於需要一個指定順序按部就班 無法適用於圖片與音樂生成 Non-autoregressive generation(NAR)\n平行運算，一次生出所有基本單位 品質問題 multi-modality AI生成會需要模型自行決策，若平行生成，可能會遇到衝突 eg: 畫一隻狗 位置一:一隻白狗，位置二:一隻黑狗 在文字接龍中很致命，會造成語意不連貫 在圖形生成方面，除了指令，還透過補充輸入一個隨機生成向量，強制給予所有平行運算單元依樣的生成依據 AR+NAR\n透過AR生成精簡版本，輸入給NAR生成細緻版本 用AR打草稿，NAR根據草稿完成 audo encoder: encoder(AR) -\u0026gt; decoder(NAR) 重複多次NAR(目前主要作法)\n小圖生大圖 有雜訊到沒有雜訊: diffusion 把每次生成錯誤的部分塗銷 也是某種auto-regressive generation, 只是生成的方式NAR，反覆將輸出重複為輸入給下一個NAR。提升速度 lec16 speculative decoding 透過預測後續token可能會是甚麼來增加產出速度 方法簡述 預測這個input經過LLM後輸出會 A + B 同時給予模型3組input: input -\u0026gt; A, input+A -\u0026gt; B, input+A+B -\u0026gt; C 根據前兩個輸入檢驗，真的如預言所猜想是A+B，那就能直接進到下個tokenC 只要有猜對其中一個就能提升效率 沒有猜中，也只是和原本的產生過程一樣，不賺不賠 預言家需求 超快速，犯錯沒關係 Non-autoregressive model 平行生成快速 compressed model 壓縮過的小模型 搜尋引擎 同時可以有多個預言家 lec17 圖片由像素構成，影片由圖片構成 如今人工智慧的輸入不會是圖片的每一個像素，而是採用encoder，把影像切成一個個patch(可能是向量或是數值)，生成後再透過decoder輸出 encoder, decoder 不只是調低解析度，其中的動作很複雜，都涵蓋了transformer 影片算是圖片增加了一個時間的維度，可以使用encoder進行更多的壓縮(如相鄰的frame一起處理) 文字生圖 訓練資料: 圖片與對應描述 使用non-autoregression，平行生成 實際使用是同時生成，而不是多個平行 因為在同個transformer中彼此有attention 評量影像生成好壞: CLIP 模型訓練過程中，給予圖片與描述，輸出為匹配分數 但實際文字能夠描述的很有限 個人化圖片生成 使用一個平常不用的符號，給予目標多次訓練 則之後就能用該符號，指定生成的樣式 文字生影片 spatio-temporal attention (3D) 同時考慮每個像素在畫面中的關係以及不同時間點該像素的關係 運算量過大需要簡化 簡化 spatial attention(2D) 僅考慮每個像素在畫面中的關係 可能會出現前後畫面不協調 temporal attention (1D) 僅考慮像素點在不同時間的的關係 會導致在畫面中不協調 結合兩者，可將原本的n^3 轉換成n^2 + n 可以再結合之前提及的多次NAR 首先產生低解析度低FPS的影片 之後幾次可以提高FPS或是提高解析度 lec18 文字生成圖片，因為文字無法完整描述影像，會有一段文字對應到多個圖片的狀況，transformer會無所適從 VAE 加入額外資訊給模型 此處的額外資訊稱為noise 資訊抽取模型 encoder 與圖片生成模型 decoder一起訓練 給予文字與圖片，encoder提取noise noise與文字輸入給decoder使起產生圖片 評斷是否與原先圖片相似 整個組合為auto encoder 於使用模型階段，這些noise的部分則是隨機產生 flow-based method 與VAE相似 只使用一個模型 VAE的encoder decoder工作內容剛好相反 訓練一個decoder模型$f$，並且是invertible VAE encoder的部分在flow中就會是$f^{-1}$ noise noise 擁有圖形的一些特徵資訊 這些noise可以被組合或改變 如對一張人臉加入笑臉noise，就能調整輸出內容為笑臉 diffusion method 此處的decoder為denoise，也是transformer 重複多次去除雜訊 訓練過程 給予圖片，圖片加上雜訊 訓練denoise model可以將有雜訊的圖片還原成圖片 generative adversarial network(GAN) 有個與CLIP相近的模型，用於圖形與文字的吻合度，稱作Discriminator 思路相反，圖片生成模型generator透過不斷修正參數生成圖片，直到通過discriminator的評斷 正因為圖片與文字並不存在一對一關係 只要令其生成的內容讓discriminator覺得好就行了，不存在標準答案 discriminator generator會交替訓練 此處的Discriminator就是reward model 可以當作plugin，與其他模型(VAE, Diffusion)進行組合使用 ","date":"2024-08-06T00:00:00+08:00","permalink":"http://shawn1251.github.io/zh-tw/post/generativeai-2024-youtube-summery/","title":"2024 李弘毅 生成式AI導論筆記"},{"content":"Mind Map\nInstructor’s name: Ali Safari Textbook: Principles of Distributed Database Systems, 4th edition, M. Tamer Özsu and Patrick Valduriez, Springer, 2020, ISBN 978-3-030-26252-5\nDistributed and Parallel Database Design fragmentation correctness\ncompletness\neach data in relation can also be found after fragmentation reconstruction\nby JOIN, the fragment can recovery to the original relation disjointness\ndata in one fragment should not also be in other fragment type\nhorizontal fragmentation (HF\nprimary horizontal (PHF\nkey points\nsimple prdicate\npredicate: key + operator + value\neg. salary \u0026gt; 1000 minterm predicate\nall possible combination of predicate\neg. loc = \u0026ldquo;France\u0026rdquo; ^ salary \u0026gt; 1000 minterm selectivities, sel(mi)\nthe percentage of records that minterm selected access frequency, acc(qi)\nhow many times the same query asked by different user cardinality, card(R)\nnumber of rows COM_MIN algorithm\ninput: a relation R, a set of simple predicates Pr\noutput: a \u0026ldquo;complete\u0026rdquo;, \u0026ldquo;minimal\u0026rdquo; set of simple predicates Pr'\nPHORIZONTAL Algorithm\ninput: a relation R, a set of predicates Pr\noutput: a set of minterm predicates M according to which relation R is to be fragmented\nderived horizontal (DHF\nBased on the fragments created by PHF, apply similar fragmentation to other related relations.\neg. after PHF, we divide \u0026ldquo;PAY\u0026rdquo; to 2 fragments. There is also a relation \u0026ldquo;EMP\u0026rdquo; related with \u0026ldquo;PAY\u0026rdquo;. We can also divide \u0026ldquo;EMP\u0026rdquo; by the same rule\nvertical fragmentation (VF\naffinity matrix\ncalculate by access frequency matrix and usage matrix bond energy algorithm (BEA\ninput: the AA matrix (attribute affinity)\noutput: the CA matrix(clustered affinity matrix)\nby changing the order what\u0026rsquo;s the most contribution I can get?\nfind the best order for columns\nhybrid fragmentation\napply both horizontal and vertical\nreconstruction\nvertical: join\nhorizontal: union\ndata distribution allocation alternatives\nnon-replicated\neach fragment resides at only one site replicated\nfully replicated\neach fragment at each site partially replicated\neach fragment at some of the sites if read-only queries \u0026raquo; update queries, replication is good\nfragment allocation\nproblem: fragments, network, application\nfind the optimal distribution\nminimal cost\nperformance\nconstraint\nresponse time\nstorage\nprocessing\ndecision variable\nXij. 1 if fragment i store in at Site j. 0 otherwise both FAP and DAP are NP-complete\nheuristic based on. about finding the best combination combined approach transaction all operations as one unit. whole or nothing ACID Atomicity\none unit Consistency\nIsolation\nDurability\nconcurrent execution increase processor and disk utilization (I/O no need CPU)\nreduced average response time\nimportant: multi tasks run in the but the result should be the same as serial running\nvalidation\nexcept read - read, all the others are conflict\ntry to move commands to see if they can be restored to the serial running format\nif the commands are conflict, it should not be moved\nserializability view serializability\nnot strict\nthe initial, update, final result should be the same as serial schedule\ncheck a schedule is serializable is NP-Complete problem\nconflict serializability\nmore strict\nconflict serializable is the sub set of serializable there is no any conflict between transactions(R/W, W/W)\ntest method\nswap non-conflicting instruction\nIf a schedule S can be transformed into a schedule S’ by a series of swaps of non-conflicting instructions, we say that S and S’ are conflict equivalent\na schedule S is conflict serializable if it is conflict equivalent to a serial schedule\nprecedence graph\ntransaction =\u0026gt; node\nconficts =\u0026gt; edge\nif graph has cycle, means not serializable\ncan do topological sorting failure rollbacks\ncascading rollback\n1 transaction failure, all the other transactions rollback recoverable schedule\nensure data consistency\nreading transaction can read data which not commit yet, but cannot commit before the writing transaction\ncascadeless schedules\nenhace recoverable schedule\nis the subset of recoverable transaction can only read data which is commited\nthe schedule which try to avoid cascading rollbacks\nconcurrency control concept the mechanism provided by the db system\nserial schedule is recoverable and cascadeless\nhave to trade off between serial schedule and concurrent schedule\nensure schedule is conflict or view serializable\nensure the schedule is rcoverable and preferably cascadeless\nto achieve these purpose, it needs a \u0026ldquo;protocol\u0026rdquo; to assure serializability\nprotocols lock-based protocols\nexclusive (X) mode\ncannot add any other lock\nonly one transaction can R/W data\nshared (S) mode\ncan add more shared lock\nmultiple read transaction can read the data at the same time\ntwo-phase locking protocol\ngrow-lockpoint-shrink\ngrow\nthe transaction acquire all the lock before access without release\ncan convert lock-S to lock-X (upgrade)\nshrink\nstart to releasing locks, cannot acquire any new lock\ncan convert lock-X to lock-S (downgrade)\ntype\nstrict two-phase locking\nkeep all the X-lock till commit/abort rigorous two-phase locking\nkeep all the locks till commit/abort ensure conflict-serializable\ncannot avoid deadlock\nstartegy\nread: if lock: read() else: if lock-X: wait() grant lock-S read()\nwrite: if lock-X: write() else: if other locks: wait() if lock-S: upgrade to lock-X else: grant lock-X write()\nlock table\nmaintain by lock manager\nlock table, record the type of lock granted or requested\nlike hash table\nvalidate before grant new lock\nif there are multiple locks, the last one can only be lock-X Graph based protocol\nalternative to two phase locking\nTree protocol\nOnly exclusive locks are allowed\nonce unlock, cannot relock\nconflict serializable\nnot gurantee recoverability\nno deadlock\ndeadlock prevention strategies wait-die\nolder may wait for younger release\nyounger never wait, rolled back instead\nwound-wait\nolder can force rollback younger\nyounger may wait for older\nfewer rollback than wait-die\ntimeout-based schemes\ndeadlock detection wait-for graph\nTi -\u0026gt; Tj: Ti is waiting for a lock held by Tj\ndeadlock if there is a cycle\ndeadlock recovery total rollback\npartial rollback\ndifficult multiple granularity can be represented as a tree\nlocks a node, also locks all the children node\nFine granularity: high concurrency, lower in tree\nCoarse granularity: low concurrency, higher in tree\nintention lock modes\n3 more lock mode than S, X\nintention-shared (IS)\nsame as S, but locking at a lower level intention-exclusive (IX)\nshared and intention-exclusive (SIX)\nallow a higher level node to be locked without having to check all descendent nodes\nthe compatibility matrix for all lock modes\nTimestamp-based protocols timestamp order = serializability order\nTimestamp-ordering protocol\nWTS(Q) (W-timestamp): the largest timestamp of any transaction that executed \u0026ldquo;write(Q)\u0026rdquo;\nRTS(Q) (R-timestamp): the largest timestamp of any transaction that executed \u0026ldquo;read(Q)\u0026rdquo;\nalgorithm\nTi = Read(Q)\nif TS(Ti) \u0026lt; WTS(Q), Reject (Ti needs the value that was already overwritten)\nif TS(Ti) \u0026gt;= WTS(Q), execute, RTS(Q) update to max(RTS(Q), TS(Ti)) (Read after latest update is accepted)\nTi = Write(Q)\nif TS(Ti) \u0026lt; RTS(Q), Reject (Ti produce a value that was needed previously)\nif TS(Ti) \u0026lt; WTS(Q), Reject (Ti try to write an obsolete value)\nelse, WTS(Q) update to TS(Ti)\ntransaction processing-2 Distributed TM Architecture serializability the condition that global transaction is serializable\neach local history should be serializable\ntwo conflicting operations should be in the same relative order in all of the local histories where they appear together\nconcurrency control algorithms Pessimistic\nTwo-Phase Locking-based (2PL)\ncentralized 2PL\nonly one 2PL scheduler in the distributed system\nlock requests are issued to the central scheduler\npros: Simple\ncons: reliability, bottle neck\ndistributed 2PL\nDeadlock locking-based algorithm may cause deadlocks\nTO based algorithm that involve waiting may cause deadlocks\nwait-for graph\nTi waits for Tj\nTi \u0026ndash;\u0026gt; Tj\nQuery Processing for one query, there may be several strategies optimization: calculate the cost, then choose the lowest one\naccess cost\ntransfer cost\nexample\nproblem\nCost of Alternatives\nComplexity of relational operations Select Project\nO(n) Project (eliminate duplicate) Group\nO(n * log n)\nsorting + check the array sequentially\nJoin Semi-Join Division Set\nO(n * log n) Cartesian Product\nO(n^2) Query Processing Methodology Query Decomposition input: Calculus query on global relations\nNormalization\nAnalysis\nSimplification\nRestructing\noutput: Algebraic query\nData Localization input: Algebraic query on distributed relations\nLocalization program\nReduction based on the fragmentation strategy\nPHF\nSelect\nBecause we have already divided the relations base on some rule. Only have to access the relations that have intersection with the query Join\nDistribute join over union\n(R1 U R2)⋈S =\u0026gt; (R1⋈S) U (R2⋈S)\nby distribute 1 join to multiple join, we can eliminate some of them that have no intersection with the query\nVF\nfind useless intermiediate relations DHF\nmix the PHF-Select and PHF-Join\nexample\nquery\neliminate by Selection join over union eliminate the empty intermediate relations Hybrid Fragmentation\nremove empty relations by selection on HF\nremove useless relations by projection on VF\ndistribute joins over unions to isolate and remove useless joins\noutput: Fragment query\nDistributed Query Optimization input: Fragment query\nFind the best global schedule\nquery optimization process\nSearch Space\nThe set of equivalent alebra expressions\nJoin Trees\nLinear join tree\nBushy join tree\nCost Model\nI/O cost + CPU cost + communication cost Search Algorithm\nexhaustive search / heuristic algorithm\nhow to \u0026ldquo;move\u0026rdquo; in the search space\nDeterministic\nstart from base relations and build plans by adding one relation at each step\nDP: BFS\nGreedy: DFS\nRandomized\ntrade optimization time for execution time\niterative improvement\n","date":"2024-07-31T00:00:00+08:00","permalink":"http://shawn1251.github.io/zh-tw/post/distributed-database-lecture-note/","title":"Distributed Database System Lecture Note"},{"content":" I2C的傳輸一樣是兩條線，但不同於TX RX，I2C只有一條SDA可以運作，另一條是時鐘線SCL SDA(serial data)，傳輸資料 SCL(serial clock)，提供時鐘脈衝 半雙工 TX RX可以同一時間傳輸與接收，稱為全雙工通信。而I2C同時間只能有一方傳輸，所以為半雙工 主從模式 同一時間只能有一方發信，為了避免衝突只能先由主機發起通訊，從機收到後再回覆 可以有多個從機 總線協議 如I2C這種可以多個設備間的通信 主機在發送的訊息開頭，會加上目標設備地址，其餘從機收到目標不是自己的訊息會選擇丟棄 同步通信 異步通信雙方有各自的時鐘，以協議的Baud Rate進行通信 但一些小型的傳感器沒有準確的晶振可以提供時鐘 所以由主機的SCL給予所有從機時鐘脈衝 ","date":"2023-11-23T00:00:00+08:00","permalink":"http://shawn1251.github.io/zh-tw/post/i2c-note/","title":"I2C筆記"},{"content":"最近在用STM32學習嵌入式開發。但由於沒有STLINK，我這邊只能使用CH340燒錄。這邊記錄了一下過程。\n需求軟體 Keil5 FlyMcu 步驟 CH340有幾個針腳 VCC: 這有一個跳線帽，可以設定5V/3.3V RX: 接收，接在GPIO的PA09 TX: 發送，接在GPIO的PA10 GND: 接地 燒錄需要的是hex檔，所以在我的IDE Keil5上要額外設定 點擊魔術棒的icon(options for target) Output \u0026gt; create hex file 之後在build後，hex file會出現在Objects資料夾中 這邊使用的燒錄軟體是FlyMcu，以下是燒錄步驟 插上CH340後，確認上方的port com為我們的CH340 Code file for online ISP \u0026gt; 選擇build好的hex file 勾選 Verify, Run After ISP complete 取消勾選 Program OptionBytes when ISP 接著就能點擊 Start ISP 如果右側訊息出現下方訊息表示成功 1 2 3 .... Write 1KB Ok,100%,@1562ms Go from 08000000 Ok 如果要再燒錄新的program，要按一下stm32上的reset按鈕 ","date":"2023-11-20T00:00:00+08:00","permalink":"http://shawn1251.github.io/zh-tw/post/stm32-ch340/","title":"使用CH340燒錄程式到STM32"},{"content":"任務調動的工具。有過基礎linux經驗的應該都知道crontab，但他有無法建立複雜任務依賴，簡單調閱日誌等缺點，這時就需要一個完善的ETL工具。本篇筆記簡單記錄我的學習過程，並將成果上傳到github repo\n另外目前airflow已經發展到v2，而網路上還能找到不少關於v1的教學，所以在學習的時候要注意版本。官方的文章在此\n特點 開源 友善的UI 有豐富的plugin 純python Getting Started 推薦快速啟動可以使用官方docker compose https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html\n1 curl -LfO \u0026#39;https://airflow.apache.org/docs/apache-airflow/2.7.3/docker-compose.yaml\u0026#39; 創建必要volume與設定airflow執行者\n1 2 mkdir -p ./dags ./logs ./plugins ./config echo -e \u0026#34;AIRFLOW_UID=$(id -u)\u0026#34; \u0026gt; .env init與執行\n1 2 docker compose up airflow-init docker compose up DAG DAG(Directed Acyclic Graph)有向無環圖。在Airflow中，DAG是一個工作流程的定義，描述了一系列的任務（Tasks）和它們之間的依賴關係。每個任務代表一個工作單元，可以是任何可以在Airflow中執行的操作，例如執行Python腳本、執行SQL查詢、調用外部API等。\nexample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from datetime import datetime, timedelta from airflow import DAG from airflow.operators.dummy_operator import DummyOperator from airflow.operators.python_operator import PythonOperator # 定義預設參數 default_args = { \u0026#39;owner\u0026#39;: \u0026#39;airflow\u0026#39;, \u0026#39;depends_on_past\u0026#39;: False, \u0026#39;start_date\u0026#39;: datetime(2023, 1, 1), \u0026#39;email_on_failure\u0026#39;: False, \u0026#39;email_on_retry\u0026#39;: False, \u0026#39;retries\u0026#39;: 1, \u0026#39;retry_delay\u0026#39;: timedelta(minutes=5), } def print_hello(): print(\u0026#34;Hello from the PythonOperator task\u0026#34;) # 定義DAG with DAG( \u0026#39;simple_dag_example\u0026#39;, default_args=default_args, description=\u0026#39;A simple example DAG\u0026#39;, schedule_interval=timedelta(days=1), # 每天執行一次 ) as dag: # 定義兩個任務，v2開始也能使用decorator start_task = DummyOperator( task_id=\u0026#39;start_task\u0026#39;, dag=dag, ) python_task = PythonOperator( task_id=\u0026#39;python_task\u0026#39;, python_callable=print_hello, dag=dag, ) # 定義任務之間的依賴關係，這樣的範例為先執行start_task後再執行python_task start_task \u0026gt;\u0026gt; python_task Scheduler Scheduler會定時去檢查DAGs的資料夾\n檢查是否有任何DAGs需要DAG Run 對DAG Run 下的task建立schedule task instance 所以要建立一個任務最直覺的方式，就是將編寫好的DAG py檔放入DAGs資料夾。可以從airflow的example中複製並修改開始，或直接參考上個section的example\nUI操作 如果scheduler已經完成update，則我們就能在ui介面上看到我們新增的DAG。關於UI的操作說明礙於平台不適合放入過多圖片，因此改提供官方的UI說明連結。 大家可以朝著最基本的操作熟悉為目標:\n檢視DAG運作情況 手動觸發DAG運作 調閱DAG執行log ","date":"2023-11-14T00:00:00+08:00","permalink":"http://shawn1251.github.io/zh-tw/post/apache-airflow/","title":"Apache Airflow 筆記"},{"content":"本篇筆記是來自心河擺渡-成瘾始于痛苦，戒瘾终于平衡！深度解读多巴胺，用身体内稳态戒瘾 。影片中有作者自己的心得分享以及他對Anna Lembke的著作Drug Dealer, MD: How Doctors Were Duped, Patients Got Hooked, and Why It’s So Hard to Stop 的一些整理。內容十分受用，特地作筆記提醒自己。\n成癮的背景 成癮的原因在於提升的多巴胺 巧克力提升55% 尼古丁提高150% 苯丙胺提高1000% 多巴胺的主要作用不是在獲得獎勵後感到快樂，而是驅動人們追求獎勵的動機 好比看到巧克力時，多巴胺會啟動腦中獎賞迴路，還沒食用就能感受到愉悅，進而產生食用的衝動 大腦中產生快樂的區域與產生痛苦的區域是重疊的 快樂與痛苦有自我調節功能，當現在快樂的同時，大腦會在另一端產生足夠的痛苦來平衡 獎賞預測誤差 因為神經適應，實際的獎勵要超出預期才會令多巴胺有好的效果 重度成癮者，感受快樂的門檻會越來越高，造成快感缺失 利用多巴胺機制 根據臨床經驗，重製大腦獎賞迴路最短需要一個月 以痛治痛 大腦會用痛苦平衡快樂，反過來用痛苦平衡快樂 根據研究，人浸泡在冷水中，血漿中的多巴胺濃度增加250%，而且持續時間超過浸泡的時間 運動是最好的選擇 物理阻斷 與成癮物徹底分開 激進誠實 前額葉皮質負責理性決策 在多巴胺獎勵迴路發生時，說謊會抑制前額葉皮質作用，停止對獎勵迴路的約束，造成大腦認為自己沒有上癮 在戒癮當中重要的一環就是重建前額葉皮質與多巴胺迴路的關係 透過\u0026quot;說實話\u0026quot;強化前額葉皮質，提高自控力 在成癮時，與自我對話或是對身邊的人宣布自己有什麼癮 ","date":"2023-11-06T00:00:00+08:00","permalink":"http://shawn1251.github.io/zh-tw/post/addiction-recovery/","title":"Addiction Recovery"},{"content":"最近在 Udemy 上學習 DevOps Beginners to Advanced with Projects。在此做一些關於 AWS 使用上的筆記。\nOriginal Application Stack 專案github\nNginx Apache Tomcat RabbitMQ Memcache Mysql 移轉目標 EC2 vm for tomcat, rabbitmq,memcache, mysql ELB nginx load balancer Autoscaling automation for vm scaling S3/EFS shared storage route53 private dns service 目標架構圖 flow of execution Create key pairs Create security groups 此處切為三組: LB(取代nginx) APP(tomcat) Backend(包含rabbitmq, memcache, mysql) Launch instance with user data 目前還是半自動化的流程 手動建立intance並且於userdata中貼上環境安裝設定的shell Update ip to name mapping in route53 這邊設定一個內部DNS，讓instance之間的溝通可以用hostname Build application from source code 這邊仍是半自動化的部分，我們在local端將java的專案build完畢 Upload to S3 bucket 使用aws cli上傳build的java war至APP instance Download artifact to Tomcat EC2 instance 之前在local端是使用key的方式來連接s3 此處instance連接S3練習使用IAM role 於IAM建立新的S3 access role 於APP instance中加入建立的role aws s3 ls即可確認是否通過 Setup ELB with https(cert from amazon certificate manager) 建立target group 須注意這邊是要通向app的8080port 建立LB 設定listen HTTP/HTTPS routing 至 target group 購買網域後於aws的certificate manager申請ssl cert secure listener 選用from ACM，使用剛剛的ssl cert Map ELB endpoint to website name in DNS 於DNS提供商(此處是用godaddy)，建立CNAME，並轉向AWS LB的domain Verify DNS的設定通常會需要一點時間 可以直接先access LB的domain檢查80有沒有APP出現 Build autoscaling group for tomcat instance autoscaling三個步驟 AMI 此處我們以app作為目標，將當前的app instance create image launch template 使用剛建立的AMI，security group 與原本的APP相同 autoscaling group attach to existing load balancer 選擇load balancer使用的target group 設定scaling policy，可以使用CPU用量或是network in/out 設定notification ","date":"2023-11-02T00:00:00+08:00","permalink":"http://shawn1251.github.io/zh-tw/post/aws-note-shift-to-cloud/","title":"AWS 筆記-專案轉移至雲端"},{"content":"最近在 Udemy 上學習 DevOps Beginners to Advanced with Projects。在此做一些關於 AWS 使用上的筆記\n建立EC2 最基本的執行單位\nName and Tag 這邊可以選用 add additional tag，多補充標籤，方便在aws console上可以快速搜尋指定的instance AMI AMI 是image，有點類似docker上取用image後run成container，但在此是run 成 instance instance type 由於目前是 free tier，只使用過 t2.micro。實際使用情境可以根據需求選擇不同等級的資源 key pair 由於aws在遠端登入時都是採用ssh key，因次在這個階段要選用此instance的key pair。 方便的是一組key pair可以被重複使用在不同instance上。可因應實際使用作key pair管理 network setting 主要在建立進出規則 通常會打開的in bound是ssh，source的部分可以加入my ip 如果是網頁服務用instance，一般會加入HTTP，並且source為anywhere configure storage storage分成很多類型。好比SSD，HDD，磁帶，也有根據IO做的特化型 advanced 此處我使用到的是 User data 這一個欄位。這部分可以直接寫上shell，方便在啟動的時候就預先裝載軟體或完成相關設定 Elastic Block Store 儲存體\nvolume type storage分成很多類型。好比SSD，HDD，磁帶，也有根據IO做的特化型 還可以選擇建立地區 aws 裡volume可以動態attach attach 在指定的instance fdisk -l 可以檢查該volume的名稱 fdisk /dev/xvdf 這個路徑可能有所不同，根據前一個指令取得的dev更換 m 可以檢視可用指令 n new partition p primary partition number: default 1 First sector: default 2048 Last sector: default w write table to disk mkfs.ext4 /dev/xvdf1: 這個指令可以根據要使用的format做更換，本處使用ext4 使用 lsblk 檢查是否出現 mount /dev/xvdf1 {target path} 就能進行掛載 如果要讓其自動掛載，請修改/etc/fstab。往後就能開機自動掛載也可使用mount -a snapshot 通常主要的資料都會使用另外的volume，好比database的data 我們可以使用 EBS 中的 snapshot 功能，為當前的volume進行備份 之後一旦有需要進行還原，就可以對指定的snapshot執行create volume from snapshot，就可以將當時的內容重新掛載到instance上 Load Balancing 以兩個執行同樣http server的instances做example\n建立target group target group 的health check用來確認instance是否正常，如果是網頁伺服器，可以設定http路徑定時對指定頁面進行請求 將目標的instances拉入target group 建立load balancer 先以 Application Load Balance 為 example security group 由於是web server，要設定inbound 為 HTTP ipv4/v6 anywhere listener \u0026amp; routing 設定為轉發到剛才創建的target group 回頭設定target group的security group 因為剛剛並沒有設定load balancer所在的security group可以access到target group。因此要將load balancer所在的security group加入target group 的security 中。 inbound HTTP custom {sg id} Cloud Watch 可以用來建立警告，此處以cpu資源作作為example\nall alarm -\u0026gt; create alarm select metric -\u0026gt; ec2 -\u0026gt; pre instance metric 尋找目標的instance id，並選取CPUUtilization 在greater than..的部分設定想要的百分比，好比60 最後，設定通知的內容還有對象email或群組 EFS 這有點類似我們linux上在用的NFS，適合作為多個instance共同的儲存空間\n建立security group 由於EFS是基於網路的連線，所以必須先設定security group 將inbound 加入 NFS，source 為目標instance的security group EFS -\u0026gt; create file system 這邊練習用的performance setting使用預設的enhanced, elastic即可。實際使用根據需求調整 Network setting這邊所有availability zone的security group 都選擇剛剛新建立給EFS用的security group。 建立access point file system 選擇剛建立的file system id 從EC2進行mount 連線EFS要一些工具，執行sudo yum install -y amazon-efs-utils安裝。這部分可以參考官方文件 接下來設定/etc/fstab 加入這一行 {file-system-id}:/ {efs-mount-point} efs _netdev,noresvport,tls,accesspoint={access-point-id} 0 0 可參考官方文件 執行mount -fav。若出現 {mount-point} : successfully mounted，就完成了 ","date":"2023-10-31T00:00:00+08:00","permalink":"http://shawn1251.github.io/zh-tw/post/aws-note/","title":"AWS 筆記"},{"content":"對於有在使用linux的初階使用者，cp mv ls 等的指令應該都用的很熟練了。這邊要筆記的是我一些常用但也花了一段時間才熟練的指令\ncat 大多數使用cat是用來顯示檔案的內容，如 cat {your file}。但其實他還有連結兩個檔案，與創建檔案寫入的功能\n連接兩檔案 cat {file1} {file2} \u0026gt; {merged file} 創建新檔案並寫入 cat \u0026gt; {your file} 他會接收你接下來的輸入並寫入檔案 所以有些自動化腳本會使用這個指令來自動化創建檔案 1 2 3 cat \u0026gt; testFile \u0026lt;\u0026lt; EOF {your content} EOF grep 1 grep -R SELINUX /etc/* -i 忽略大小寫 -R 包含子資料夾 -v 反向，輸出關鍵字以外的內容 cut 可以用來快速提取固定格式檔案中的內容。好比/etc/passwd 中，可以看見內容都是以\u0026quot;:\u0026ldquo;做分隔\n1 2 root:x:0:0:root:/root:/bin/bash vagrant:x:1000:1000::/home/vagrant:/bin/bash 此時我們用cut指令\n1 cut -d: -f1 /etc/passwd -d 是切分的參數，後面接著\u0026rdquo;:\u0026ldquo;代表要以\u0026rdquo;:\u0026ldquo;做分隔 -f1 是要提取切分後的field 1的意思，此處取出的部分會是username 出來的output就會是\n1 2 root vagrant awk 前一個指令cut適用於有適合的separator，當變化較複雜時我們使用awk。如果是上述的例子改用awk改寫會是:\n1 awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39; /etc/passwd -F 是用來定義sepeartor {print $1} 是用來定義輸出，此處是輸出切分後的第一位 sed 用來做文字取代，注意的是他是對stream做取代，所以不會覆蓋到原本文件。舉例如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 echo \u0026#34;this is a book.\u0026#34; \u0026gt; test # 建立一個範例文字 sed \u0026#39;s/book/dog/g\u0026#39; test \u0026gt; this is a dog. # 他將文字輸出的book替換成dog # s 為search # g 為global # test 可以替換成\u0026#39;*\u0026#39;，更改複數個檔案 cat test \u0026gt; this is a book. # 原本的檔案沒有變化 # 如果要覆蓋，可以加上 -i sed -i \u0026#39;s/book/dog/g\u0026#39; test cat test \u0026gt; this is a dog. redirection 我們在對linux下指令的輸出預設是在螢幕上，而我們也可以將輸出進行導向轉移到file。 以下有幾個重點:\n\u0026gt; 會對輸出對象覆蓋，\u0026gt;\u0026gt; 是append 1是stdout, 2是stderr 可以使用\u0026amp;對所有輸出都進行導向 1 2 3 4 5 6 7 8 9 # 當目標是導出stdout，不用特別加上1 ls \u0026gt;\u0026gt; tmpfile # lss 為不存在指令，bash會報錯，此時可以將他報錯的stderr進行導出。 lss 2\u0026gt;\u0026gt; tmpfile # \u0026amp;符號代表將stdout stderr都導出 ls \u0026amp;\u0026gt;\u0026gt; tmpfile lss \u0026amp;\u0026gt;\u0026gt; tmpfile pipe 使用pipe |將輸出作為下個指令的輸入。\n1 2 3 4 5 # 我們將ls輸出給wc計算行數 ls | wc -l # 將free輸出的mem欄位提取出來 free | grep -i Mem ","date":"2023-10-30T00:00:00+08:00","permalink":"http://shawn1251.github.io/zh-tw/post/shell-cmd-filter/","title":"常用於filter的shell command "},{"content":"一個command line tool方便自動化啟動VM。vagrant本身不是hypervisor，而是基於hypervisor上的一層應用，令使用者可以利用他快速架設VM於hypervisor上。本身也不需要OS image，設定檔Vagrantfile中指定的image(box)會從vagrant cloud取用。在Vagrantfile中定義所需要的參數，即可使用vagrant up啟動\n常用指令 vagrant init {box name} vagrant up vagrant ssh vagrant halt vagrant destroy 流程 create folder create Vagrantfile vagrant up vagrant ssh vagrant halt/destroy 啟動範例 請在這邊找尋自己想要啟動的 box https://app.vagrantup.com/boxes/search\n1 2 3 vagrant init {box name} vagrant up vagrant global-status Vagrantfile 的 provision Vagrantfile 中有一段 provision，可以用於設定vm第一次啟動前要執行的指令，好比安裝指定的軟體。可以參考官方對Vagrantfile的說明\n1 2 3 4 5 6 # 這邊以在provision階段安裝apache2 server為例 config.vm.provision \u0026#34;shell\u0026#34;, inline: \u0026lt;\u0026lt;-SHELL apt-get update apt-get install -y apache2 SHELL ","date":"2023-10-28T00:00:00+08:00","permalink":"http://shawn1251.github.io/zh-tw/post/vagrant-intro/","title":"vagrant 筆記"},{"content":"chocolatey 是windows上一個安裝軟體的工具。我以前都是使用macOS為主，chocolatey就像是mac上的brew一樣。有了他，安裝軟體變得方便許多。\n安裝 1 2 3 4 5 6 7 Get-ExecutionPolicy # 若為 restricted，以系統管理員執行powershell後輸入以下指令 Set-ExecutionPolicy AllSigned # 輸入Y or A 完成權限設定 Set-ExecutionPolicy Bypass -Scope Process -Force; iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://chocolatey.org/install.ps1\u0026#39;)) # 最後執行`choco`來確認是否有安裝成功 choco 使用 我們可以前往 https://community.chocolatey.org/packages 網站上搜尋目標的軟體 好比我們搜尋virtual box，就可以看見他將指令寫在後方\n1 choco install virtualbox 執行指令後就會自動安裝了!\n","date":"2023-10-24T06:42:46+08:00","permalink":"http://shawn1251.github.io/zh-tw/post/install-chocolatey/","title":"安裝chocolatey"},{"content":"本篇文章用來記錄這次以Hugo建立Blog後發佈到GitHub Page的過程。 GitHub本身有個免費的個人網站服務稱作GitHub Page。只要將想要發布的網頁內容上傳到指定格式的repository即可。\n事前準備 GitHub 帳號 安裝 git 你的目標網站 建立 點擊repository，New 在repository name這個欄位填入\u0026quot;{你的帳號}\u0026quot;.github.io 點擊 create repository 上傳 接下來我們就要將本地端的網站推送到github上，若沒有網站僅是想要做測試，可以簡單建立一個index.html做測試\n1 2 3 4 5 6 7 8 9 10 11 # 為當前網站建立git git init # 加入stage並commit git add . git commit -m \u0026#34;first commit\u0026#34; # 建立main分支 git branch -M main # 將遠端repository加入設定並命名為origin git remote add origin https://github.com/{你的帳號}/{你的帳號}.github.io.git # 將當前專案push到github上 git push -u origin main 檢視 正常的話，前往https://{你的帳號}.github.io就能看到剛才推上去的網頁了!\n以Hugo為例，建立網站並上傳 承接上一篇以Hugo建立第一篇Post，我們可以利用GitHub Page對成果進行發布。 記得更改config中的baseURL\n1 2 3 4 5 6 7 8 9 10 11 # build hugo # 進入靜態網站的資料夾 cd public # 承上說明不贅述 git init git add . git commit -m \u0026#34;first commit\u0026#34; git branch -M main git remote add origin https://github.com/{你的帳號}/{你的帳號}.github.io.git git push -u origin main ","date":"2023-10-21T06:42:46+08:00","permalink":"http://shawn1251.github.io/zh-tw/post/setting-your-first-githubpage/","title":"設定github page"},{"content":"這次心血來潮決定將一些過往的筆記進行整理，在尋找平台的時候參考了友人的建議選擇了Hugo搭配github page。以下就來記錄一下過程。\nHugo 簡單介紹一下Hugo， Hugo 是用Golang開發的靜態網站產生工具。靜態網站不依賴後端，速度快又不必架設資料庫，特別適合開發展示用的網站。以前很流行使用wordpress架設個人網站， 相較於這種功能豐富的CMS，若需求單純，其實更加推薦使用靜態網站，同樣類型的工具還有 Hexo 與 Jekyll。\nHugo的本身由Golang開發，所以我們在使用的時候僅需要安裝編譯好的Hugo執行檔，不用再安裝其他相依語言如ruby、js。我們可以先瀏覽一下別人建立好的 Hugo template 想像一下之後的作品效果。\ninstallation 我們這邊參照官方的教學 https://gohugo.io/getting-started/quick-start/\n安裝Hugo 請根據自己的作業系統選擇 安裝方法。 我是使用ubuntu系統，這邊預設已經安裝git，安裝語法如下。\n1 2 3 4 # 先安裝sass套件 sudo snap install dart-sass # 安裝hugo sudo snap install hugo 安裝完成後可以執行檢查版本\n1 hugo --version 嘗試建立第一個專案 1 2 3 4 5 6 7 8 9 10 # 建立專案 hugo new site quickstart cd quickstart git init # 加入theme ananke 作為git的submodule，方便與原專案分離更新 git submodule add https://github.com/theNewDynamic/gohugo-theme-ananke.git themes/ananke # 對當前專案指定使用ananke作為theme echo \u0026#34;theme = \u0026#39;ananke\u0026#39;\u0026#34; \u0026gt;\u0026gt; hugo.toml # 執行web server顯示成果 hugo server 加入內容 執行上述步驟後應該已經能夠看到一個首頁黑白的簡單homepage了。 接著我們要加入個人的內容，我們可以使用hugo內建的指令\n1 2 # 建立一個叫做my-first-post的post hugo new content posts/my-first-post.md 此時我們再去content/posts/下就會看到出現了一個md檔案\n1 2 3 4 5 +++ title = \u0026#39;My First Post\u0026#39; date = 2023-10-20T21:37:17+08:00 draft = true +++ 與空白專案的內容不同，他的開頭會有這樣的文字。須注意這是hugo的markdown必要的metadata，如果刪除了就不會出現在首頁上了。 我們接著加上幾行自己的內容。參照 markdown語法教學\n1 2 3 4 5 6 7 8 +++ title = \u0026#39;My First Post\u0026#39; date = 2023-10-20T21:37:17+08:00 draft = false +++ # hello world hello 這邊須注意要將draft改回false，否則hugo server會不顯示，相對的需要使用hugo server -D的模式才會顯示草稿內容。\n發布 只要執行指令hugo就能開始根據內容進行打包建置。結果會在public資料夾中。如果你剛好也有python3，我們可以執行內建的http server做簡單測試。\n1 2 cd public python3 -m http.server 預設會在port 8000上，我們打開瀏覽器localhost:8000就能看到本次建置的靜態網站了。\n問題 我要如何客製化模板? 一般來說模板都有自己的document可以提供客製化需求。如這個blog使用的是 stack 。我在這個模板中遇到的問題就是要加入不存在theme當中的幾個icon。所以我要做的步驟會是前往原 repository fork 一份到自己的repository下，再進行客製化加入icon。\n找到了喜歡的模板不知道怎麼開始? 通常模板專案下有基礎的 example site 可以參考他使怎麼使用這個模板的。以 stack 為例，他存在exampleSite這個資料夾，裡面涵蓋了content與config.yaml等，可以直接將他複製出來到專案目錄中檢視。\n","date":"2023-10-20T00:00:00+08:00","permalink":"http://shawn1251.github.io/zh-tw/post/first-post/","title":"First Post with Hugo"}]